IN THE MORNING:
- tune 3,11 and 16 with bcel?
- Fix test, test_zs counts
- Find out if, with Pre ZS R@30 with gnews, the improvements are of a specific subset/type of predicates.
- put on results for vg predcls
- Then show the pre-predicate results, and justify the selection of a subset of predicates

- focus on other types of predicates, see the behavior
   NOTE: spatial predicates are all the same, semantically-wise
- complete scan on vg (vrd_trainer.py?)
- ? zs with respect to new predicates!


- Scan SoftEmbRescore with only_sem 0,1,2,3 and all_feats
- Poi vedi se funziona meglio con certi lr
	With mode 1 (no logit), it can be tuned. A relu afterwards?
  Mode 0 (logit) seems to f* everything up. Maybe a linear layer afterwards?


# TODO: filter out relationships between the same object?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Use @20 instead of R@30 and compare to other works
https://arxiv.org/pdf/2004.06193.pdf
https://kevinstan.github.io/data/aesg_report.pdf
https://arxiv.org/pdf/2004.06193.pdf


Anotnym
https://arxiv.org/pdf/1605.07766.pdf
http://tcci.ccf.org.cn/conference/2018/papers/141.pdf

- Show the results with a randomly initialized untrained model.

- create the matrix manually?
	"on", "above", "on the top of",
	"next to", "beside", "near", "by", "adjacent to", "against",
	"in", "inside", "beneath",
	"under", "below",
	"contain", ? "outside of"
	"behind",
	"in the front of",
	"on the left of",
	"on the right of",
	"across"
	Then try to fuse these and create matrix manually?

- Then I thought, I can use this value to rescale all of the scores:
		if the value is high (it is likely that the pair is in a relationship),
		all of the scores for this object pair are going to be boosted,
		if the value is low (it is unlikely that the object pair is interacting in any way),
		they are going to be low.




New idea:
 look for "Letâ€™s build a proper (yet simple" here: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e
 They have single parameters. Well, implement the linear projection for a predicate there. Beforee that, check if it's already there somewhere int he code.
 Do they claim it is already there, in the paper?




 To give example numbers, the model learns to give a 80% R@100 score without any input,
  and multi-modality only helps bringing it up to 93-95%. Also, it looks like one modality (vis, sem, spat)
  does most of the work, while the others do not help so much.
 Now, we implemented two ways of using the embeddings to improve the results,
  but we saw no improvement when used with all the modalities together.
  So right now we thought we could try introducing our predicate semantics ideas without using multi-modal features.
  For instance, with no input.
 But we would like an embedding model where antonyms have cosine similarity -1. Do you know where to find it?


https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/
