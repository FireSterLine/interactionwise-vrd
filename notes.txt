Maybe, title: Class Semantic Awareness for Visual Relationship Detection

So we believe that the metrics are weak in this sense.

- Introduce a new metric (inspired from https://arxiv.org/pdf/2004.00436.pdf),
  consisting of a variation of R@50 and R@100 where the semantics of the predicates is taken into account.
  Some cosine distance-based metric.

- create the matrix manually?
	"on", "above", "on the top of",
	"next to", "beside", "near", "by", "adjacent to", "against",
	"in", "inside", "beneath",
	"under", "below",
	"contain", "outside of"
	"behind",
	"in the front of",
	"on the left of",
	"on the right of",
	"across"
	Then try to fuse these and create matrix manually?

- Then I thought, I can use this value to rescale all of the scores:
		if the value is high (it is likely that the pair is in a relationship),
		all of the scores for this object pair are going to be boosted,
		if the value is low (it is unlikely that the object pair is interacting in any way),
		they are going to be low.


DSR:
  One direct way to dealwith incomplete predicate annotation is to take the unla-
  beled relationships as negative samples and solve the rela
  tionship detection problem using a multi-label classification
  framework based on cross-entropy loss

  In this paper, we propose a deep neural network frame-work with structural ranking loss to tackle the visual rela-tionship detection problem

  Moreover, we integrate the structural losswith the probability of the predicates conditioned on its sub-
  ject and object to further reduce the impact of incompleteannotations

  MultiLabelMarginLoss: multi-label ranking loss

  Long-tail Classification (e.g read a little bit of https://arxiv.org/pdf/2004.00436.pdf)



New idea:
 look for "Letâ€™s build a proper (yet simple" here: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e
 They have single parameters. Well, implement the linear projection for a predicate there. Beforee that, check if it's already there somewhere int he code.
 Do they claim it is already there, in the paper?

 To give example numbers, the model learns to give a 80% R@100 score without any input,
  and multi-modality only helps bringing it up to 93-95%. Also, it looks like one modality (vis, sem, spat)
  does most of the work, while the others do not help so much.
 Now, we implemented two ways of using the embeddings to improve the results,
  but we saw no improvement when used with all the modalities together.
  So right now we thought we could try introducing our predicate semantics ideas without using multi-modal features.
  For instance, with no input.
 But we would like an embedding model where antonyms have cosine similarity -1. Do you know where to find it?


https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
