
- show predicates' co-occurrence matrix (for each so pair, just s or just o)
- the ones with synonyms are?


- there are different types of predicates and word embeddings might behave differently for them.
- they do, the two methods seem to work.
- this makes sense because the spatial predicates
- antonym problem for spatial predicates
- separate the results for predicate semantics and embeddings
- do the same with VG
- show sum of the predicate rescoring matrix, or normalize it
- give RELCLS in the end

IN THE MORNING:
- check the big scan, note that some of the things may be wrong (e.g 1: check!). Also check epoch 5!

- Show the results with a randomly initialized untrained model.
- put on results for vg predcls
- complete scan on vg?
- focus on other types of predicates, see the behavior
   NOTE: spatial predicates are all the same, semantically-wise

- fix 3 SoftEmbRescore and Scan SoftEmbRescore with only_sem 0,1,2,3 and all_feats
- Poi vedi se funziona meglio con certi lr
	With mode 1 (no logit), it can be tuned. A relu afterwards?
  Mode 0 (logit) seems to f* everything up. Maybe a linear layer afterwards?

- what if you just use the label as categorical vector as feature vector of the objects?

- look up the swap thingy + swap file
- Mention that there are about 0.5 fluctuations

- are close: beside, next to, adjacent to, near
- on

- Long-tail Classification (e.g read a little bit of https://arxiv.org/pdf/2004.00436.pdf)

*** Chapter 4: ***

My main difficulty here is "seeing the forest and not just the trees". I feel that I need at least a clear section structure, but hopefully also some discussions of the various results, before I can see what you are trying to tell here, so that I can react and give my feedback.

You have many results and tables and to help the reader understand the logic of this chapter, I think it would be a good idea to start the results chapter with some sort of roadmap.

As I don't yet know the story you are trying to tell, I can't determine which results are the main ones and which are less important. But if you feel that there are too many numbers and tables, you might consider moving some tables into an appendix. You don't have to put everything that you have done into chapter 4. It's enough to include the results that illustrate your main points.

(Trivial point about tables: might look a bit more visually appealing if you don't use 4 decimal positions in some of the tables...)

You should probably have an additional conclusion chapter. (That is, 4.3 and 4.4 should become chapter 5.)


WRITING:

Abstract
- It would be nice to add as last sentence your achievement. Eg. we improved the existing model by decreasing the error by 4%.
- And then, either: in this work we find that we can leverage word embeddings for the purpose; or we clearly state the hypothesis: can we leverage word embeddings for this

Methods
- the results should show the results obtained with models described in methods, e.g explain the blind model

Discussion
- Make points about results, re-establish the research, show the limitations, future research and recommendation

Results
- be less humble in the results
- ...So we believe that the metrics are weak in this sense. In fact, this may be a side offect of using ``softened metrics'' (see section...)
- Use @20 instead of R@30 and compare to other works
    https://arxiv.org/pdf/2004.06193.pdf
    https://kevinstan.github.io/data/aesg_report.pdf
    https://arxiv.org/pdf/2004.06193.pdf
- When showing the results per-predicate, signal what predicates were not in the embedding model
